Edge cases: “U.S.A.” or “Dr. Smith” might split incorrectly. (Can fix later with nltk.sent_tokenize or spacy.)

If len(sentences) is very small (like 1–2), this might create tiny chunks → embeddings could be weak. You may want a min chunk length filter (e.g., only embed chunks with >50 chars).
